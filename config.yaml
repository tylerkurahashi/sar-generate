base_dir: /workspace

dm:
  seed: 1208
  batch_size: 1
  image_resolution: 256
  dataset_name: umbra-w256-s64
  data_dir: ${base_dir}/dataset/${dm.dataset_name}
  output_dir: ${base_dir}/output/${model.name}_res${dm.image_resolution}_b${dm.batch_size}
  ckpt_filename: best
  fold_cfg_path: ${base_dir}/fold.yaml

hp:
  resume_from_checkpoint: null
  training:
    lr: 1e-5
    epochs: 1000
    ema_decay: 0.9999

  pl_trainer:
    accelerator: gpu
    num_nodes: 1
    benchmark: true
    precision: "16-mixed"
    strategy: ddp
    num_sanity_val_steps: 0
    max_epochs: ${hp.training.epochs}

model:
  name: unet2d
  _target_: diffusers.UNet2DModel
  params:
    sample_size: 256
    in_channels: 1
    out_channels: 1
    layers_per_block: 2
    block_out_channels: [128, 128, 256, 256, 512, 512]
    down_block_types:
      - DownBlock2D
      - DownBlock2D
      - DownBlock2D
      - DownBlock2D
      - AttnDownBlock2D
      - DownBlock2D
    up_block_types:
      - UpBlock2D
      - AttnUpBlock2D
      - UpBlock2D
      - UpBlock2D
      - UpBlock2D
      - UpBlock2D

noise:
  _target_: diffusers.schedulers.DDPMScheduler
  params:
    num_train_timesteps: 1000
    beta_start: 0.0001
    beta_end: 0.02
    beta_schedule: linear

log:
  _target_: pl.loggers.WandbLogger
  params:
    project: umbra-ddpm
    entity: tylerkurahashi
    name: v1-unet2d-w256-s64
    save_dir: ./logs
    offline: false
    